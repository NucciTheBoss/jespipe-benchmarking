{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6ea9fe",
   "metadata": {},
   "source": [
    "# 10 models, 1 attack, and 1 plot simulation benchmark\n",
    "***\n",
    "\n",
    "- **Data Sets:** Energy Appliances Weather; Parkinsons Telemonitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c7ce2",
   "metadata": {},
   "source": [
    "## Necessary Imports:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from jespipe.plugin.attack.attack import Attack\n",
    "from jespipe.plugin.manip.manip import Manipulation\n",
    "from jespipe.plugin.train.build import Build\n",
    "from jespipe.plugin.train.evaluate import Evaluate\n",
    "from jespipe.plugin.train.fit import Fit\n",
    "from jespipe.plugin.train.predict import Predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0702b78",
   "metadata": {},
   "source": [
    "## Class Definition:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47762e5",
   "metadata": {},
   "source": [
    "### Candlestick Manipulation Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2072858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandlestickManip(Manipulation):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Candlestick Manipulation class to facilitate the Candlestick trend \n",
    "        extraction technique on many-to-one datasets with no header row.\n",
    "        Target feature holds index position -1 in the passed dataset.\n",
    "        ### Parameters:\n",
    "        - :param parameters: Parameter dictionary.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - manipulate (abstract): Perform Candlestick trend extraction on passed dataset.\n",
    "        - private\n",
    "          - _preproc_candlestick: Internal Candlestick trend extraction preprocessing method for passed dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(parameters[\"dataset\"], header=None)\n",
    "        self.manip_params = parameters[\"manip_params\"]\n",
    "\n",
    "    def manipulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform Candlestick trend extraction technique on passed dataset.\n",
    "        \"\"\"\n",
    "        features, labels = self._preproc_candlestick()\n",
    "        recomb = pd.concat([pd.DataFrame(features), pd.DataFrame(labels)], axis=1)\n",
    "        return recomb\n",
    "\n",
    "    def _preproc_candlestick(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal Candlestick trend extraction preprocessing method for passed dataset.\n",
    "        Splits DataFrame into features and labels.\n",
    "        ### Returns:\n",
    "        :return: Tuple with dataset features at index 0 and labels at index 1.\n",
    "        \"\"\"\n",
    "        # Features for model training\n",
    "        features = np.array(self.dataset)[:, :-1]\n",
    "\n",
    "        # Labels\n",
    "        labels = np.array(self.dataset)[:, -1]\n",
    "\n",
    "        new_features = np.zeros((int(features.shape[0] / self.manip_params[\"time_interval\"]), int(features.shape[1] * 4)))\n",
    "        new_labels = np.zeros((int(labels.shape[0] / self.manip_params[\"time_interval\"]),))\n",
    "        for feature_ind in range(features.shape[1]):\n",
    "            new_feature_ind = feature_ind * 4\n",
    "            new_row_ind = 0\n",
    "            for row_ind in range(0, features.shape[0] - self.manip_params[\"time_interval\"], self.manip_params[\"time_interval\"]):\n",
    "                # Find the 'open' value\n",
    "                open_value = features[row_ind, feature_ind]\n",
    "                new_features[new_row_ind, new_feature_ind] = open_value\n",
    "\n",
    "                # Find the 'close' value\n",
    "                end_ind = int(row_ind + (self.manip_params[\"time_interval\"]-1))\n",
    "                close_value = features[end_ind, feature_ind]\n",
    "                new_features[new_row_ind, new_feature_ind + 1] = close_value\n",
    "\n",
    "                # Find the 'high' value\n",
    "                high_value = np.max(features[row_ind:end_ind, feature_ind])\n",
    "                new_features[new_row_ind, new_feature_ind + 2] = high_value\n",
    "\n",
    "                # Find the 'low' value\n",
    "                low_value = np.min(features[row_ind:end_ind, feature_ind])\n",
    "                new_features[new_row_ind, new_feature_ind + 3] = low_value\n",
    "\n",
    "                # Save the label -- to change probably\n",
    "                new_labels[new_row_ind] = labels[row_ind]\n",
    "\n",
    "                # Update row index for new_features\n",
    "                new_row_ind += 1\n",
    "\n",
    "        return new_features, new_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ceff87",
   "metadata": {},
   "source": [
    "### PCA Manipulation Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e24c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAManip(Manipulation):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        PCA Manipulation class to facilitate the PCA dimensionality \n",
    "        reduction technique on many-to-one datasets with no header row.\n",
    "        Target feature holds index position -1 in the passed dataset.\n",
    "        ### Parameters:\n",
    "        - :param parameters: Parameter dictionary.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - manipulate (abstract): Perform PCA dimensionality reduction on passed dataset.\n",
    "        - private\n",
    "          - _preproc_xgb: Internal PCA dimensionality reduction preprocessing method for passed dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(parameters[\"dataset\"], header=None)\n",
    "        self.manip_params = parameters[\"manip_params\"]\n",
    "\n",
    "    def manipulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform PCA dimensionality reduction technique on passed dataset.\n",
    "        \"\"\"\n",
    "        features, labels = self._preproc_pca()\n",
    "        recomb = pd.concat([pd.DataFrame(features), pd.DataFrame(labels)], axis=1)\n",
    "        return recomb\n",
    "\n",
    "    def _preproc_pca(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal PCA dimensionality reduction preprocessing method for passed dataset.\n",
    "        Splits DataFrame into features and labels.\n",
    "        ### Returns:\n",
    "        :return: Tuple with dataset features at index 0 and labels at index 1.\n",
    "        \"\"\"\n",
    "        # Features for training\n",
    "        features = np.array(self.dataset)[:, :-1]\n",
    "\n",
    "        # Labels\n",
    "        labels = np.array(self.dataset)[:, -1]\n",
    "\n",
    "        new_features = PCA(n_components=self.manip_params[\"n_features\"]).fit_transform(features)\n",
    "\n",
    "        return new_features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707fa8d",
   "metadata": {},
   "source": [
    "### RandomForest Manipulation Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestManip(Manipulation):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        RandomForest Manipulation class to facilitate the RandomForst \n",
    "        feature selection technique on many-to-one datasets with no header row.\n",
    "        Target feature holds index position -1 in the passed dataset.\n",
    "        ### Parameters:\n",
    "        - :param parameters: Parameter dictionary.\n",
    "        ### Methods:\n",
    "        - public \n",
    "          - manipulate (abstract): Perform vanilla manipulation on passed dataset.\n",
    "        - private \n",
    "          - _preproc_randomforest: Internal vanilla preprocessing method for passed dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(parameters[\"dataset\"], header=None)\n",
    "\n",
    "    def manipulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform RandomForest feature selection technique on passed dataset.\n",
    "        \"\"\"\n",
    "        features, labels = self._preproc_randomforest()\n",
    "        recomb = pd.concat([pd.DataFrame(features), pd.DataFrame(labels)], axis=1)\n",
    "        return recomb\n",
    "\n",
    "    def _preproc_randomforest(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal RandomForest feature selection preprocessing method for passed dataset.\n",
    "        Splits DataFrame into features and labels.\n",
    "        ### Returns:\n",
    "        :return: Tuple with dataset features at index 0 and labels at index 1. \n",
    "        \"\"\"\n",
    "        # Features for training\n",
    "        features = np.array(self.dataset)[:, :-1]\n",
    "\n",
    "        # Labels\n",
    "        labels = np.array(self.dataset)[:, -1]\n",
    "\n",
    "        sel = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
    "        sel.fit(features, labels.astype('int'))\n",
    "\n",
    "        features_to_select = sel.get_support(indices=True)\n",
    "        # print('Features to select: ', features_to_select)\n",
    "\n",
    "        new_features = features[:, features_to_select]\n",
    "\n",
    "        return new_features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a57511",
   "metadata": {},
   "source": [
    "### Vanilla Manipulation Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaManip(Manipulation):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Vanilla Manipulation class to facilitate vanilla data manipulation\n",
    "        on many-to-one datasets with no header row. Target feature holds \n",
    "        index position -1 in the passed dataset.\n",
    "        ### Parameters:\n",
    "        - :param parameters: Parameter dictionary.\n",
    "        ### Methods:\n",
    "        - public \n",
    "          - manipulate (abstract): Perform vanilla manipulation on passed dataset.\n",
    "        - private \n",
    "          - _preproc_vanilla: Internal vanilla preprocessing method for passed dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(parameters[\"dataset\"], header=None)\n",
    "        self.manip_tag = parameters[\"manip_tag\"]\n",
    "        self.save_path = parameters[\"save_path\"]\n",
    "        self.tmp_path = parameters[\"tmp_path\"]\n",
    "\n",
    "    def manipulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform vanilla manipulation on passed dataset.\n",
    "        \"\"\"\n",
    "        features, labels = self._preproc_vanilla()\n",
    "        recomb = pd.concat([pd.DataFrame(features), pd.DataFrame(labels)], axis=1)\n",
    "        return recomb\n",
    "\n",
    "    def _preproc_vanilla(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal vanilla preprocessing method for passed dataset.\n",
    "        Splits DataFrame into features and labels.\n",
    "        ### Returns:\n",
    "        :return: Tuple with dataset features at index 0 and labels at index 1.\n",
    "        \"\"\"\n",
    "        # Features for model training\n",
    "        features = np.array(self.dataset)[:, :-1]\n",
    "\n",
    "        # Labels\n",
    "        labels = np.array(self.dataset)[:, -1]\n",
    "\n",
    "        return features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602a104",
   "metadata": {},
   "source": [
    "### XGB Manipulation Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97064ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBManip(Manipulation):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        XGBoost Manipulation class to facilitate the XGBoost feature \n",
    "        selection technique on many-to-one datasets with no header row.\n",
    "        Target feature holds index position -1 in the passed dataset.\n",
    "        ### Parameters:\n",
    "        - :param parameters: Parameter dictionary.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - manipulate (abstract): Perform XGBoost feature selection on passed dataset.\n",
    "        - private\n",
    "          - _preproc_xgb: Internal XGBoost feature selection preprocessing method for passed dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(parameters[\"dataset\"], header=None)\n",
    "        self.manip_tag = parameters[\"manip_tag\"]\n",
    "        self.manip_params = parameters[\"manip_params\"]\n",
    "        self.save_path = parameters[\"save_path\"]\n",
    "        self.tmp_path = parameters[\"tmp_path\"]\n",
    "\n",
    "    def manipulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform XGBoost feature selection technique on passed dataset.\n",
    "        \"\"\"\n",
    "        features, labels = self._preproc_xgb()\n",
    "        recomb = pd.concat([pd.DataFrame(features), pd.DataFrame(labels)], axis=1)\n",
    "        return recomb\n",
    "\n",
    "    def _preproc_xgb(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal XGBoost feature selection preprocessing method for passed dataset.\n",
    "        Splits DataFrame into features and labels.\n",
    "        ### Returns:\n",
    "        :return: Tuple with dataset features at index 0 and labels at index 1. \n",
    "        \"\"\"\n",
    "        # Features for training\n",
    "        features = np.array(self.dataset)[:, :-1]\n",
    "\n",
    "        # Labels\n",
    "        labels = np.array(self.dataset)[:, -1]\n",
    "\n",
    "        feature_train, feature_test, labels_train, labels_test = train_test_split(features, labels)\n",
    "\n",
    "        # Training with best gamma\n",
    "        regressor = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            gamma=1.5,\n",
    "            max_depth=self.manip_params[\"n_features\"]\n",
    "        )\n",
    "\n",
    "        regressor.fit(feature_train, labels_train)\n",
    "\n",
    "        feature_importance = regressor.feature_importances_\n",
    "\n",
    "        features_to_select = feature_importance.argsort()[-self.manip_params[\"n_features\"]:][::-1]\n",
    "\n",
    "        new_features = features[:, features_to_select]\n",
    "\n",
    "        return new_features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16376036",
   "metadata": {},
   "source": [
    "### RNN LSTM Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee85ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildLSTM(Build):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Build class to initialize Sequential LSTM model.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param parameters: Parameter dictionary sent by Jespipe.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - build_model (abstract): Build LSTM RNN model using uncompromised data.\n",
    "        - private\n",
    "          - _load_data: Internal method for loading/splitting the data into the training and testing data.\n",
    "        \"\"\"\n",
    "        self.dataset_name = parameters[\"dataset_name\"]\n",
    "        self.dataframe = parameters[\"dataframe\"]\n",
    "        self.model_params = parameters[\"model_params\"]\n",
    "\n",
    "    def build_model(self) -> Tuple[Sequential, Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Build LSTM RNN model using uncompromised data.\n",
    "        ### Returns:\n",
    "        :return: (model, (feat_train, label_train, feat_test, label_test))\n",
    "        - Positional value of each index in the tuple:\n",
    "          - 0: An unfitted Sequential LSTM model.\n",
    "          - 1: The training dataset split into training features, training labels, \n",
    "          test features, and test labels.\n",
    "        \"\"\"\n",
    "        sequence_length = self.model_params[\"sequence_length\"]\n",
    "        feature_count = self.dataframe.shape[1]-1\n",
    "        learn_rate = self.model_params[\"learning_rate\"]\n",
    "\n",
    "        # Split into training and test\n",
    "        feat_train, label_train, feat_test, label_test = self._load_data(self.dataframe, sequence_length, feature_count)\n",
    "\n",
    "        # Start building the model using Keras\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(5):\n",
    "            model.add(LSTM(input_shape=feat_train.shape[1:], units=30, return_sequences=True))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(LSTM(30, return_sequences=False))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(units=1))\n",
    "        opt = Adam(learning_rate=learn_rate)\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=opt, metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Return created model and training data and testing data\n",
    "        return model, (feat_train, label_train, feat_test, label_test)\n",
    "\n",
    "    def _load_data(self, data: pd.DataFrame, seq_len: int, feature_count: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal method for loading/splitting the data into the training and testing data.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param data: Passed dataset to split into training and testing features and labels.\n",
    "        :param seq_len: User-controlled hyperparameter for LSTM architecture.\n",
    "        :param feature_count: Number of features in the passed dataset.\n",
    "        ### Returns:\n",
    "        :return: (x_train, y_train, x_test, y_test)\n",
    "        - Positional value of each index in the tuple:\n",
    "          - 0: Training features.\n",
    "          - 1: Training labels.\n",
    "          - 2: Test features.\n",
    "          - 3: Test labels.\n",
    "        \"\"\"\n",
    "        result = np.zeros((len(data) - seq_len, seq_len, feature_count+1))\n",
    "\n",
    "        # Sequence lengths remain together\n",
    "        # (i.e, 6 consecutive candles stay together at all times if seq_len=6)\n",
    "        for index in range(len(data) - seq_len):\n",
    "            result[index] = data[index: index + seq_len]\n",
    "\n",
    "        # Shuffling with for reproducable results\n",
    "        np.random.seed(2020)\n",
    "\n",
    "        # In-place shuffling for saving space\n",
    "        np.random.shuffle(result)\n",
    "\n",
    "        # Amount of data to train on. Train: 85%; Test: 15%\n",
    "        row = len(result) * 0.85\n",
    "        train = result[:int(row), :]\n",
    "\n",
    "        x_train = train[:, :, :-1]\n",
    "        y_train = train[:, -1][:, -1]\n",
    "        x_test = result[int(row):, :, :-1]\n",
    "        y_test = result[int(row):, -1][:, -1]\n",
    "\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], feature_count))\n",
    "        x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], feature_count))\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "class FitLSTM(Fit):\n",
    "    def __init__(self, model: Sequential, feat_train: np.ndarray, \n",
    "                    label_train: np.ndarray, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Fit class to facilitate fitting Sequential LSTM model to training data.\n",
    "        \n",
    "        ### Paramters:\n",
    "        :param model: Sequential LSTM model to fit to training data.\n",
    "        :param feat_train: Training features.\n",
    "        :param label_train: Training labels.\n",
    "        :param parameters: Parameter dictionary sent by Jespipe.\n",
    "        \n",
    "        ### Methods:\n",
    "        - public\n",
    "          - model_fit (abstract): Fit Sequential LSTM model using user-specified hyperparameters.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.feat_train = feat_train\n",
    "        self.label_train = label_train\n",
    "        self.model_params = parameters[\"model_params\"]\n",
    "        self.batch_size = self.model_params[\"batch_size\"]\n",
    "        self.epochs = self.model_params[\"epochs\"]\n",
    "        self.validation_split = self.model_params[\"validation_split\"]\n",
    "        self.verbose = self.model_params[\"verbose\"]\n",
    "\n",
    "    def model_fit(self) -> None:\n",
    "        \"\"\"\n",
    "        Fit Sequential LSTM model using user-specified hyperparameters.\n",
    "        \"\"\"\n",
    "        self.model.fit(\n",
    "            self.feat_train,\n",
    "            self.label_train,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            validation_split=self.validation_split,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "class PredictLSTM(Predict):\n",
    "    def __init__(self, model: Sequential, predictee: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Prediction class to facilitate making predictions with Sequential LSTM model.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param model: Sequential LSTM model to make predictions with.\n",
    "        :param predictee: Data to make prediction on.\n",
    "        \n",
    "        ### Methods:\n",
    "        - public\n",
    "          - model_predict (abstract): Make prediction on data using Sequential LSTM model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.predictee = predictee\n",
    "\n",
    "    def model_predict(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make prediction on data using Sequential LSTM model.\n",
    "        ### Returns:\n",
    "        :return: Sequential LSTM model's prediction\n",
    "        \"\"\"\n",
    "        prediction = self.model.predict(self.predictee)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class EvaluateLSTM(Evaluate):\n",
    "    def __init__(self, feature_test: np.ndarray, label_test: np.ndarray, \n",
    "                    model_to_eval: Sequential) -> None:\n",
    "        \"\"\"\n",
    "        Evaluation class to facilitate evalutions predictions made by fitted Sequential LSTM model.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param feature_test: Test features.\n",
    "        :param label_test: Test labels.\n",
    "        :param model_to_eval: Sequential LSTM model to evaulate.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - model_evaluate (abstract): Evaluate the mean squared error and root mean squared error of \n",
    "          the Sequential LSTM model's prediction.\n",
    "        - private:\n",
    "          - _eval_mse: Internal method to evaluate the mean squared error \n",
    "          of the Sequential LSTM model's prediction.\n",
    "          - _eval_rmse: Internal method to evaluate the root \n",
    "          mean squared error of the Sequential LSTM model's prediction\n",
    "        \"\"\"\n",
    "        self.feature_test = feature_test\n",
    "        self.label_test = label_test\n",
    "        self.model_to_eval = model_to_eval\n",
    "\n",
    "    def model_evaluate(self) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the mean squared error and root mean squared error of \n",
    "        the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: (mse, rmse)\n",
    "        - Positional value of each index in the tuple:\n",
    "          - 0: Mean squared error of model's prediction.\n",
    "          - 1: Root mean squared error of model's prediction.\n",
    "          - 2: Scatter index of model's prediction.\n",
    "          - 3: Mean absolute error of model's prediction.\n",
    "        \"\"\"\n",
    "        mse = self._eval_mse(); rmse = self._eval_rmse(mse)\n",
    "        return mse, rmse, self._eval_scatter_index(rmse), self._eval_mean_absolute_error()\n",
    "\n",
    "    def _eval_mse(self) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the mean squared error \n",
    "        of the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: Mean squared error of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        score = self.model_to_eval.evaluate(self.feature_test, self.label_test, verbose=0)\n",
    "\n",
    "        # Index 1 is MSE; index 0 is loss\n",
    "        return score[1]\n",
    "\n",
    "    def _eval_rmse(self, mse: float) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the root mean \n",
    "        squared error of the Sequential LSTM model's prediction.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param mse: Mean squared error of the Sequential LSTM model's prediction.\n",
    "        \n",
    "        ### Returns:\n",
    "        :return: Root mean squared error of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        return np.sqrt(mse)\n",
    "\n",
    "    def _eval_scatter_index(self, rmse: float) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the scatter index\n",
    "        of the Sequential LSTM model's prediction.\n",
    "        ### Parameters:\n",
    "        :param rmse: Root mean squared error of the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: Scatter index of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        return np.multiply(np.divide(rmse, np.mean(self.feature_test)), 100)\n",
    "\n",
    "    def _eval_mean_absolute_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the mean absolute error\n",
    "        of the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: Mean absolute error of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        mae = MeanAbsoluteError()\n",
    "        return mae(self.label_test, self.model_to_eval.predict(self.feature_test)).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be69d5d",
   "metadata": {},
   "source": [
    "### Carlini & Wagner L<sub>2</sub> Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarliniL2(Attack):\n",
    "    \"\"\"\n",
    "    This is a modified version of the L_2 optimized attack of Carlini and Wagner (2016).\n",
    "    It has been modified to fit time series regression problems.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, features: np.ndarray, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Create a Carlini&Wagner L_2 attack instance.\n",
    "        ### Parameters:\n",
    "        :param model: System file path to trained regressor model.\n",
    "        :param model_test_features: Test features to use for adversarial example generation.\n",
    "        :param parameters: Parameter dictionary for the attack.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - attack (abstract): Launch L_2 attack on the given time series data.\n",
    "        - private\n",
    "          - _generate: Internal method to perform the L_2 attack on the given time series data.\n",
    "          - _generate_batch: Internal method to generate batched adversarial samples and return them in an array.\n",
    "        \"\"\"\n",
    "        self.model = load_model(model)\n",
    "        self.features = features\n",
    "        self.min_change = parameters[\"change\"]\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.max_iter = parameters[\"max_iter\"]\n",
    "        self.binary_search_steps = parameters[\"binary_search_steps\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.initial_const = parameters[\"initial_const\"]\n",
    "        self.sequence_length = parameters[\"sequence_length\"]\n",
    "        self.verbose = parameters[\"verbose\"]\n",
    "\n",
    "    def attack(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Launch L_2 attack on the given time series data.\n",
    "        ### Returns:\n",
    "        :return: An array holding the adversarial examples.\n",
    "        \"\"\"\n",
    "        return self._generate(self.features)\n",
    "\n",
    "    def _generate(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Internal method to perform the L_2 attack on the given time series data.\n",
    "        ### Parameters:\n",
    "        :param x: An array with the original inputs to be attacked.\n",
    "        ### Returns:\n",
    "        :return: An array holding the adversarial examples.\n",
    "        \"\"\"\n",
    "        pred = self.model.predict(x)\n",
    "        self.mean = pred.mean()\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        x_adv = np.zeros(x.shape)\n",
    "        nb_batches = int(np.ceil(x.shape[0] / float(self.batch_size)))\n",
    "        for i in trange(nb_batches, desc=\"C&W L_2\", disable = not self.verbose):\n",
    "            index = i * self.batch_size\n",
    "            x_adv[index:index+self.batch_size] = (self._generate_batch(x[index:index+self.batch_size]))\n",
    "        print(x_adv.shape)\n",
    "        return x_adv\n",
    "    \n",
    "    def _generate_batch(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Internal method to generate batched adversarial samples and return them in an array.\n",
    "        ### Parameters:\n",
    "        :param x: An array with the batched original inputs to be attacked.\n",
    "        ### Returns:\n",
    "        :return: An array holding the batched adversarial examples.\n",
    "        \"\"\"\n",
    "        # Initialize constant for binary search:\n",
    "        c_current = np.ones(x.shape[0]) * self.initial_const\n",
    "        c_best = np.zeros(x.shape[0])\n",
    "        \n",
    "        # Initialize placeholders for best l2 distance and attack found so far\n",
    "        best_l2dist = np.inf * np.ones(x.shape[0])\n",
    "        best_x_adv = x.copy()\n",
    "        \n",
    "        pred = self.model.predict(x)\n",
    "        \n",
    "        # Initialize boolean to decide if advesarial examples should predict above or below original\n",
    "        # Since the adv examples are normalized between [0,1], adv examples that predict values approaching 0 or 1 are difficult to generate, hence the bool\n",
    "        mean = pred.mean()\n",
    "        above = (mean <= self.mean)\n",
    "        \n",
    "        if above:\n",
    "            if mean + self.min_change > 0.9:\n",
    "                above = False\n",
    "        else:\n",
    "            if mean - self.min_change < 0.1:\n",
    "                above = True\n",
    "        \n",
    "        for bss in range(self.binary_search_steps):\n",
    "            \n",
    "            # Initialize variable to optimize\n",
    "            w = tf.Variable(np.zeros(x.shape), trainable=True, dtype=tf.float32)\n",
    "            \n",
    "            for i_iter in range(self.max_iter):\n",
    "                \n",
    "                # Calculate loss\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(w)\n",
    "                    \n",
    "                    # Generate adversarial examples using w\n",
    "                    x_adv = (tf.tanh(w) + 1.0) / 2.0\n",
    "                    pred_adv = self.model(x_adv)\n",
    "                    \n",
    "                    # Calculate distance using the l2 metric\n",
    "                    square_diff = tf.square(tf.subtract(x, x_adv))\n",
    "                    l2dist = tf.reduce_sum(tf.reduce_sum(square_diff, axis=2), keepdims=(True))\n",
    "                    \n",
    "                    # Loss depends if adv prediction is meant to be above or below the benign prediction\n",
    "                    if above:\n",
    "                        f_sum = tf.add(tf.add(pred, self.min_change), tf.negative(pred_adv))\n",
    "                    else:\n",
    "                        f_sum = tf.add(tf.add(tf.negative(pred), self.min_change), pred_adv)\n",
    "                    c_loss = tf.multiply(c_current, tf.maximum(f_sum, tf.zeros(x_adv.shape[0])))\n",
    "                    \n",
    "                    # Add the two sums from the loss function\n",
    "                    loss = tf.add(l2dist, c_loss)\n",
    "                \n",
    "                # Calculate loss gradient w.r.t our optimization variable w \n",
    "                gradients = tape.gradient(loss, w)\n",
    "                \n",
    "                # Update w\n",
    "                w = tf.subtract(w, tf.multiply(self.learning_rate, gradients))\n",
    "                \n",
    "                # Calculate l2dist and generate new adversarial predictions\n",
    "                x_adv = x_adv.numpy()\n",
    "                pred_adv = self.model.predict(x_adv)\n",
    "                l2dist = np.sum(np.square(x - x_adv).reshape(x.shape[0], -1), axis=1)\n",
    "                \n",
    "                # Update adversarial examples if new best is found\n",
    "                for e in range(x.shape[0]): \n",
    "                    if above:\n",
    "                        if pred_adv[e] >= pred[e] + self.min_change and l2dist[e] <= best_l2dist[e]:\n",
    "                            best_x_adv[e] = x_adv[e]\n",
    "                            best_l2dist[e] = l2dist[e]\n",
    "                    else:\n",
    "                        if pred_adv[e] <= pred[e] - self.min_change and l2dist[e] <= best_l2dist[e]:\n",
    "                            best_x_adv[e] = x_adv[e]\n",
    "                            best_l2dist[e] = l2dist[e]\n",
    "            \n",
    "            pred_adv = self.model.predict(x_adv)\n",
    "            \n",
    "            # Update constant c using modified binary search\n",
    "            for e in range(x.shape[0]):\n",
    "                if above:\n",
    "                    if pred_adv[e] >= pred[e] + self.min_change:\n",
    "                        c_best[e] = c_current[e]\n",
    "                        c_current[e] /= 2\n",
    "                    else:\n",
    "                        if c_best[e] == 0:\n",
    "                            c_current[e] *= 10\n",
    "                        else:\n",
    "                            c_current[e] = (c_current[e] + c_best[e]) / 2\n",
    "                else:\n",
    "                    if pred_adv[e] <= pred[e] - self.min_change:\n",
    "                        c_best[e] = c_current[e]\n",
    "                        c_current[e] /= 2\n",
    "                    else:\n",
    "                        if c_best[e] == 0:\n",
    "                            c_current[e] *= 10\n",
    "                        else:\n",
    "                            c_current[e] = (c_current[e] + c_best[e]) / 2\n",
    "        return best_x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97eda2",
   "metadata": {},
   "source": [
    "### Plotting Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d69bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a74e08",
   "metadata": {},
   "source": [
    "## Simulation:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4766ee",
   "metadata": {},
   "source": [
    "### Training stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7962d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cbe21fc",
   "metadata": {},
   "source": [
    "### Attack stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1da231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b35aec15",
   "metadata": {},
   "source": [
    "### Cleaning stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487ef7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb0098a4",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3951d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
