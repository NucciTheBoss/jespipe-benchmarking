{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147ad74b",
   "metadata": {},
   "source": [
    "# 1 model, 1 attack, and 1 plot simulation benchmark:\n",
    "***\n",
    "\n",
    "- **Data Set:** Air Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf13e2",
   "metadata": {},
   "source": [
    "## Necessary Imports:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "import jespipe.plugin.save as save\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from jespipe.plugin.attack.attack import Attack\n",
    "from jespipe.plugin.manip.manip import Manipulation\n",
    "from jespipe.plugin.train.build import Build\n",
    "from jespipe.plugin.train.evaluate import Evaluate\n",
    "from jespipe.plugin.train.fit import Fit\n",
    "from jespipe.plugin.train.predict import Predict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cade3",
   "metadata": {},
   "source": [
    "## Class Definition:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e220c",
   "metadata": {},
   "source": [
    "### Vanilla Manipulation Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaManip(Manipulation):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Vanilla Manipulation class to facilitate vanilla data manipulation\n",
    "        on many-to-one datasets with no header row. Target feature holds \n",
    "        index position -1 in the passed dataset.\n",
    "        ### Parameters:\n",
    "        - :param parameters: Parameter dictionary.\n",
    "        ### Methods:\n",
    "        - public \n",
    "          - manipulate (abstract): Perform vanilla manipulation on passed dataset.\n",
    "        - private \n",
    "          - _preproc_vanilla: Internal vanilla preprocessing method for passed dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(parameters[\"dataset\"], header=None)\n",
    "\n",
    "    def manipulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform vanilla manipulation on passed dataset.\n",
    "        \"\"\"\n",
    "        features, labels = self._preproc_vanilla()\n",
    "        recomb = pd.concat([pd.DataFrame(features), pd.DataFrame(labels)], axis=1)\n",
    "        return recomb\n",
    "\n",
    "    def _preproc_vanilla(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal vanilla preprocessing method for passed dataset.\n",
    "        Splits DataFrame into features and labels.\n",
    "        ### Returns:\n",
    "        :return: Tuple with dataset features at index 0 and labels at index 1.\n",
    "        \"\"\"\n",
    "        # Features for model training\n",
    "        features = np.array(self.dataset)[:, :-1]\n",
    "\n",
    "        # Labels\n",
    "        labels = np.array(self.dataset)[:, -1]\n",
    "\n",
    "        return features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183b43a",
   "metadata": {},
   "source": [
    "### RNN LSTM Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildLSTM(Build):\n",
    "    def __init__(self, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Build class to initialize Sequential LSTM model.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param parameters: Parameter dictionary sent by Jespipe.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - build_model (abstract): Build LSTM RNN model using uncompromised data.\n",
    "        - private\n",
    "          - _load_data: Internal method for loading/splitting the data into the training and testing data.\n",
    "        \"\"\"\n",
    "        self.dataset_name = parameters[\"dataset_name\"]\n",
    "        self.dataframe = parameters[\"dataframe\"]\n",
    "        self.model_params = parameters[\"model_params\"]\n",
    "\n",
    "    def build_model(self) -> Tuple[Sequential, Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Build LSTM RNN model using uncompromised data.\n",
    "        ### Returns:\n",
    "        :return: (model, (feat_train, label_train, feat_test, label_test))\n",
    "        - Positional value of each index in the tuple:\n",
    "          - 0: An unfitted Sequential LSTM model.\n",
    "          - 1: The training dataset split into training features, training labels, \n",
    "          test features, and test labels.\n",
    "        \"\"\"\n",
    "        sequence_length = self.model_params[\"sequence_length\"]\n",
    "        feature_count = self.dataframe.shape[1]-1\n",
    "        learn_rate = self.model_params[\"learning_rate\"]\n",
    "\n",
    "        # Split into training and test\n",
    "        feat_train, label_train, feat_test, label_test = self._load_data(self.dataframe, sequence_length, feature_count)\n",
    "\n",
    "        # Start building the model using Keras\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(5):\n",
    "            model.add(LSTM(input_shape=feat_train.shape[1:], units=30, return_sequences=True))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(LSTM(30, return_sequences=False))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(units=1))\n",
    "        opt = Adam(learning_rate=learn_rate)\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=opt, metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Return created model and training data and testing data\n",
    "        return model, (feat_train, label_train, feat_test, label_test)\n",
    "\n",
    "    def _load_data(self, data: pd.DataFrame, seq_len: int, feature_count: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Internal method for loading/splitting the data into the training and testing data.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param data: Passed dataset to split into training and testing features and labels.\n",
    "        :param seq_len: User-controlled hyperparameter for LSTM architecture.\n",
    "        :param feature_count: Number of features in the passed dataset.\n",
    "        ### Returns:\n",
    "        :return: (x_train, y_train, x_test, y_test)\n",
    "        - Positional value of each index in the tuple:\n",
    "          - 0: Training features.\n",
    "          - 1: Training labels.\n",
    "          - 2: Test features.\n",
    "          - 3: Test labels.\n",
    "        \"\"\"\n",
    "        result = np.zeros((len(data) - seq_len, seq_len, feature_count+1))\n",
    "\n",
    "        # Sequence lengths remain together\n",
    "        # (i.e, 6 consecutive candles stay together at all times if seq_len=6)\n",
    "        for index in range(len(data) - seq_len):\n",
    "            result[index] = data[index: index + seq_len]\n",
    "\n",
    "        # Shuffling with for reproducable results\n",
    "        np.random.seed(2020)\n",
    "\n",
    "        # In-place shuffling for saving space\n",
    "        np.random.shuffle(result)\n",
    "\n",
    "        # Amount of data to train on. Train: 85%; Test: 15%\n",
    "        row = len(result) * 0.85\n",
    "        train = result[:int(row), :]\n",
    "\n",
    "        x_train = train[:, :, :-1]\n",
    "        y_train = train[:, -1][:, -1]\n",
    "        x_test = result[int(row):, :, :-1]\n",
    "        y_test = result[int(row):, -1][:, -1]\n",
    "\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], feature_count))\n",
    "        x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], feature_count))\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "class FitLSTM(Fit):\n",
    "    def __init__(self, model: Sequential, feat_train: np.ndarray, \n",
    "                    label_train: np.ndarray, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Fit class to facilitate fitting Sequential LSTM model to training data.\n",
    "        \n",
    "        ### Paramters:\n",
    "        :param model: Sequential LSTM model to fit to training data.\n",
    "        :param feat_train: Training features.\n",
    "        :param label_train: Training labels.\n",
    "        :param parameters: Parameter dictionary sent by Jespipe.\n",
    "        \n",
    "        ### Methods:\n",
    "        - public\n",
    "          - model_fit (abstract): Fit Sequential LSTM model using user-specified hyperparameters.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.feat_train = feat_train\n",
    "        self.label_train = label_train\n",
    "        self.model_params = parameters[\"model_params\"]\n",
    "        self.batch_size = self.model_params[\"batch_size\"]\n",
    "        self.epochs = self.model_params[\"epochs\"]\n",
    "        self.validation_split = self.model_params[\"validation_split\"]\n",
    "        self.verbose = self.model_params[\"verbose\"]\n",
    "\n",
    "    def model_fit(self) -> None:\n",
    "        \"\"\"\n",
    "        Fit Sequential LSTM model using user-specified hyperparameters.\n",
    "        \"\"\"\n",
    "        self.model.fit(\n",
    "            self.feat_train,\n",
    "            self.label_train,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            validation_split=self.validation_split,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "class PredictLSTM(Predict):\n",
    "    def __init__(self, model: Sequential, predictee: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Prediction class to facilitate making predictions with Sequential LSTM model.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param model: Sequential LSTM model to make predictions with.\n",
    "        :param predictee: Data to make prediction on.\n",
    "        \n",
    "        ### Methods:\n",
    "        - public\n",
    "          - model_predict (abstract): Make prediction on data using Sequential LSTM model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.predictee = predictee\n",
    "\n",
    "    def model_predict(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make prediction on data using Sequential LSTM model.\n",
    "        ### Returns:\n",
    "        :return: Sequential LSTM model's prediction\n",
    "        \"\"\"\n",
    "        prediction = self.model.predict(self.predictee)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class EvaluateLSTM(Evaluate):\n",
    "    def __init__(self, feature_test: np.ndarray, label_test: np.ndarray, \n",
    "                    model_to_eval: Sequential) -> None:\n",
    "        \"\"\"\n",
    "        Evaluation class to facilitate evalutions predictions made by fitted Sequential LSTM model.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param feature_test: Test features.\n",
    "        :param label_test: Test labels.\n",
    "        :param model_to_eval: Sequential LSTM model to evaulate.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - model_evaluate (abstract): Evaluate the mean squared error and root mean squared error of \n",
    "          the Sequential LSTM model's prediction.\n",
    "        - private:\n",
    "          - _eval_mse: Internal method to evaluate the mean squared error \n",
    "          of the Sequential LSTM model's prediction.\n",
    "          - _eval_rmse: Internal method to evaluate the root \n",
    "          mean squared error of the Sequential LSTM model's prediction\n",
    "        \"\"\"\n",
    "        self.feature_test = feature_test\n",
    "        self.label_test = label_test\n",
    "        self.model_to_eval = model_to_eval\n",
    "\n",
    "    def model_evaluate(self) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the mean squared error and root mean squared error of \n",
    "        the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: (mse, rmse)\n",
    "        - Positional value of each index in the tuple:\n",
    "          - 0: Mean squared error of model's prediction.\n",
    "          - 1: Root mean squared error of model's prediction.\n",
    "          - 2: Scatter index of model's prediction.\n",
    "          - 3: Mean absolute error of model's prediction.\n",
    "        \"\"\"\n",
    "        mse = self._eval_mse(); rmse = self._eval_rmse(mse)\n",
    "        return mse, rmse, self._eval_scatter_index(rmse), self._eval_mean_absolute_error()\n",
    "\n",
    "    def _eval_mse(self) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the mean squared error \n",
    "        of the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: Mean squared error of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        score = self.model_to_eval.evaluate(self.feature_test, self.label_test, verbose=0)\n",
    "\n",
    "        # Index 1 is MSE; index 0 is loss\n",
    "        return score[1]\n",
    "\n",
    "    def _eval_rmse(self, mse: float) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the root mean \n",
    "        squared error of the Sequential LSTM model's prediction.\n",
    "        \n",
    "        ### Parameters:\n",
    "        :param mse: Mean squared error of the Sequential LSTM model's prediction.\n",
    "        \n",
    "        ### Returns:\n",
    "        :return: Root mean squared error of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        return np.sqrt(mse)\n",
    "\n",
    "    def _eval_scatter_index(self, rmse: float) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the scatter index\n",
    "        of the Sequential LSTM model's prediction.\n",
    "        ### Parameters:\n",
    "        :param rmse: Root mean squared error of the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: Scatter index of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        return np.multiply(np.divide(rmse, np.mean(self.feature_test)), 100)\n",
    "\n",
    "    def _eval_mean_absolute_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Internal method to evaluate the mean absolute error\n",
    "        of the Sequential LSTM model's prediction.\n",
    "        ### Returns:\n",
    "        :return: Mean absolute error of the Sequential LSTM model's prediction.\n",
    "        \"\"\"\n",
    "        mae = MeanAbsoluteError()\n",
    "        return mae(self.label_test, self.model_to_eval.predict(self.feature_test)).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf8559",
   "metadata": {},
   "source": [
    "### Carlini & Wagner L<sub>2</sub> Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarliniL2(Attack):\n",
    "    \"\"\"\n",
    "    This is a modified version of the L_2 optimized attack of Carlini and Wagner (2016).\n",
    "    It has been modified to fit time series regression problems.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, features: np.ndarray, parameters: dict) -> None:\n",
    "        \"\"\"\n",
    "        Create a Carlini&Wagner L_2 attack instance.\n",
    "        ### Parameters:\n",
    "        :param model: System file path to trained regressor model.\n",
    "        :param model_test_features: Test features to use for adversarial example generation.\n",
    "        :param parameters: Parameter dictionary for the attack.\n",
    "        ### Methods:\n",
    "        - public\n",
    "          - attack (abstract): Launch L_2 attack on the given time series data.\n",
    "        - private\n",
    "          - _generate: Internal method to perform the L_2 attack on the given time series data.\n",
    "          - _generate_batch: Internal method to generate batched adversarial samples and return them in an array.\n",
    "        \"\"\"\n",
    "        self.model = load_model(model)\n",
    "        self.features = features\n",
    "        self.min_change = parameters[\"change\"]\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.max_iter = parameters[\"max_iter\"]\n",
    "        self.binary_search_steps = parameters[\"binary_search_steps\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.initial_const = parameters[\"initial_const\"]\n",
    "        self.sequence_length = parameters[\"sequence_length\"]\n",
    "        self.verbose = parameters[\"verbose\"]\n",
    "\n",
    "    def attack(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Launch L_2 attack on the given time series data.\n",
    "        ### Returns:\n",
    "        :return: An array holding the adversarial examples.\n",
    "        \"\"\"\n",
    "        return self._generate(self.features)\n",
    "\n",
    "    def _generate(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Internal method to perform the L_2 attack on the given time series data.\n",
    "        ### Parameters:\n",
    "        :param x: An array with the original inputs to be attacked.\n",
    "        ### Returns:\n",
    "        :return: An array holding the adversarial examples.\n",
    "        \"\"\"\n",
    "        pred = self.model.predict(x)\n",
    "        self.mean = pred.mean()\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        x_adv = np.zeros(x.shape)\n",
    "        nb_batches = int(np.ceil(x.shape[0] / float(self.batch_size)))\n",
    "        for i in trange(nb_batches, desc=\"C&W L_2\", disable = not self.verbose):\n",
    "            index = i * self.batch_size\n",
    "            x_adv[index:index+self.batch_size] = (self._generate_batch(x[index:index+self.batch_size]))\n",
    "        print(x_adv.shape)\n",
    "        return x_adv\n",
    "    \n",
    "    def _generate_batch(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Internal method to generate batched adversarial samples and return them in an array.\n",
    "        ### Parameters:\n",
    "        :param x: An array with the batched original inputs to be attacked.\n",
    "        ### Returns:\n",
    "        :return: An array holding the batched adversarial examples.\n",
    "        \"\"\"\n",
    "        # Initialize constant for binary search:\n",
    "        c_current = np.ones(x.shape[0]) * self.initial_const\n",
    "        c_best = np.zeros(x.shape[0])\n",
    "        \n",
    "        # Initialize placeholders for best l2 distance and attack found so far\n",
    "        best_l2dist = np.inf * np.ones(x.shape[0])\n",
    "        best_x_adv = x.copy()\n",
    "        \n",
    "        pred = self.model.predict(x)\n",
    "        \n",
    "        # Initialize boolean to decide if advesarial examples should predict above or below original\n",
    "        # Since the adv examples are normalized between [0,1], adv examples that predict values approaching 0 or 1 are difficult to generate, hence the bool\n",
    "        mean = pred.mean()\n",
    "        above = (mean <= self.mean)\n",
    "        \n",
    "        if above:\n",
    "            if mean + self.min_change > 0.9:\n",
    "                above = False\n",
    "        else:\n",
    "            if mean - self.min_change < 0.1:\n",
    "                above = True\n",
    "        \n",
    "        for bss in range(self.binary_search_steps):\n",
    "            \n",
    "            # Initialize variable to optimize\n",
    "            w = tf.Variable(np.zeros(x.shape), trainable=True, dtype=tf.float32)\n",
    "            \n",
    "            for i_iter in range(self.max_iter):\n",
    "                \n",
    "                # Calculate loss\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(w)\n",
    "                    \n",
    "                    # Generate adversarial examples using w\n",
    "                    x_adv = (tf.tanh(w) + 1.0) / 2.0\n",
    "                    pred_adv = self.model(x_adv)\n",
    "                    \n",
    "                    # Calculate distance using the l2 metric\n",
    "                    square_diff = tf.square(tf.subtract(x, x_adv))\n",
    "                    l2dist = tf.reduce_sum(tf.reduce_sum(square_diff, axis=2), keepdims=(True))\n",
    "                    \n",
    "                    # Loss depends if adv prediction is meant to be above or below the benign prediction\n",
    "                    if above:\n",
    "                        f_sum = tf.add(tf.add(pred, self.min_change), tf.negative(pred_adv))\n",
    "                    else:\n",
    "                        f_sum = tf.add(tf.add(tf.negative(pred), self.min_change), pred_adv)\n",
    "                    c_loss = tf.multiply(c_current, tf.maximum(f_sum, tf.zeros(x_adv.shape[0])))\n",
    "                    \n",
    "                    # Add the two sums from the loss function\n",
    "                    loss = tf.add(l2dist, c_loss)\n",
    "                \n",
    "                # Calculate loss gradient w.r.t our optimization variable w \n",
    "                gradients = tape.gradient(loss, w)\n",
    "                \n",
    "                # Update w\n",
    "                w = tf.subtract(w, tf.multiply(self.learning_rate, gradients))\n",
    "                \n",
    "                # Calculate l2dist and generate new adversarial predictions\n",
    "                x_adv = x_adv.numpy()\n",
    "                pred_adv = self.model.predict(x_adv)\n",
    "                l2dist = np.sum(np.square(x - x_adv).reshape(x.shape[0], -1), axis=1)\n",
    "                \n",
    "                # Update adversarial examples if new best is found\n",
    "                for e in range(x.shape[0]): \n",
    "                    if above:\n",
    "                        if pred_adv[e] >= pred[e] + self.min_change and l2dist[e] <= best_l2dist[e]:\n",
    "                            best_x_adv[e] = x_adv[e]\n",
    "                            best_l2dist[e] = l2dist[e]\n",
    "                    else:\n",
    "                        if pred_adv[e] <= pred[e] - self.min_change and l2dist[e] <= best_l2dist[e]:\n",
    "                            best_x_adv[e] = x_adv[e]\n",
    "                            best_l2dist[e] = l2dist[e]\n",
    "            \n",
    "            pred_adv = self.model.predict(x_adv)\n",
    "            \n",
    "            # Update constant c using modified binary search\n",
    "            for e in range(x.shape[0]):\n",
    "                if above:\n",
    "                    if pred_adv[e] >= pred[e] + self.min_change:\n",
    "                        c_best[e] = c_current[e]\n",
    "                        c_current[e] /= 2\n",
    "                    else:\n",
    "                        if c_best[e] == 0:\n",
    "                            c_current[e] *= 10\n",
    "                        else:\n",
    "                            c_current[e] = (c_current[e] + c_best[e]) / 2\n",
    "                else:\n",
    "                    if pred_adv[e] <= pred[e] - self.min_change:\n",
    "                        c_best[e] = c_current[e]\n",
    "                        c_current[e] /= 2\n",
    "                    else:\n",
    "                        if c_best[e] == 0:\n",
    "                            c_current[e] *= 10\n",
    "                        else:\n",
    "                            c_current[e] = (c_current[e] + c_best[e]) / 2\n",
    "        return best_x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f10d4",
   "metadata": {},
   "source": [
    "### Plotting Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b283f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c2a6941",
   "metadata": {},
   "source": [
    "## Simulation:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96637ba8",
   "metadata": {},
   "source": [
    "### Training stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/air-quality/models/vanilla1/data\", exist_ok=True)\n",
    "os.makedirs(\"data/air-quality/models/vanilla1/stat\", exist_ok=True)\n",
    "\n",
    "# Use vanilla manipulation on data\n",
    "manip_params = {\"dataset\": \"tmp/air-quality/air-quality.csv\"}\n",
    "vanillamanip = VanillaManip(manip_params)\n",
    "dataframe = vanillamanip.manipulate()\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "dataframe = pd.DataFrame(sc.fit_transform(dataframe))\n",
    "\n",
    "# Read in model hyperparameters\n",
    "fin = open(\".config.json\", \"rt\"); config_file = fin.read(); fin.close()\n",
    "config_file = json.loads(config_file)\n",
    "\n",
    "# Train model\n",
    "model_params = config_file.get(\"air-quality\")\n",
    "params = {\"dataset_name\": \"air-quality\", \"dataframe\": dataframe, \"model_params\": model_params}\n",
    "\n",
    "lstm_build = BuildLSTM(params)\n",
    "model, data = lstm_build.build_model()\n",
    "\n",
    "lstm_fit = FitLSTM(model, data[0], data[1], params)\n",
    "lstm_fit.model_fit()\n",
    "\n",
    "save.dictionary(\"data/air-quality/vanilla1\", \"model_parameters\", model_params)\n",
    "save.features(\"data/air-quality/vanilla1\", data[2]); save.labels(\"data/air-quality/vanilla1\", data[3])\n",
    "save.compress_dataframe(\"data/air-quality/vanilla1/data\", \"baseline-data-normalized\", dataframe)\n",
    "with open(\"data/air-quality/vanilla1/model_summary.txt\", \"wt\") as fout: lstm_fit.model.summary(print_fn=lambda x: fout.write(x + \"\\n\"))\n",
    "lstm_fit.model.save(\"data/air-quality/vanilla1/air-quality-vanilla1.h5\", include_optimizer=True)\n",
    "\n",
    "lstm_predict = PredictLSTM(lstm_fit.model, data[2])\n",
    "pred = lstm_predict.model_predict()\n",
    "save.compress_dataframe(\"data/air-quality/vanilla1/data\", \"baseline-prediction\", pd.DataFrame(pred))\n",
    "\n",
    "lstm_evaluate = EvaluateLSTM(data[2], data[3], lstm_fit.model)\n",
    "mse, rmse, scatter_index, mae = lstm_evaluate.model_evaluate()\n",
    "\n",
    "log_dict = {\"0.0\": {\"mse\": mse, \"rmse\": rmse, \"scatter_index\": scatter_index, \"mae\": mae}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de4d60",
   "metadata": {},
   "source": [
    "### Attack stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4d44a",
   "metadata": {},
   "source": [
    "### Cleaning stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725c1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74efc2c0",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370bccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
